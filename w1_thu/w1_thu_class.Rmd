---
title: "Class Note - Week 2"
author: "Allen Ho"
date: "7/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
library(dplyr)
library(httr)
library(rvest)
library(stringr)
library(tidytext)
setwd('~/Documents/DSP/Data_Science_Programming/w1_thu/')
```

# Data Combination 

### Data Source

- [World Bank Unemployment Data (1991-2017)](https://www.kaggle.com/uddipta/world-bank-unemployment-data-19912017/version/1 )
- [Suicide Rates Overview 1985 to 2016](https://www.kaggle.com/russellyates88/suicide-rates-overview-1985-to-2016) 

### New Data

- __country__: country name
- __year__: year
- __region__: the region of the country
- __unemploy__: the unemployment rate of the country in that year
- __gdp_year__: the gdp of the country in that year
- __sr_15_24__: the suicide number in 15~24 years old population (among 100k people)
- __sr_25_34__: the suicide number in 25~34 years old population (among 100k people)
- __sr_35_54__: the suicide number in 35~54 years old population (among 100k people)

### Code

load the csv files

```{r load data}
r_suicide = read.csv('./data/suicide.csv')
r_unemploy = read.csv('./data/unemployment.csv')
```

define the function `process_suicide_data()` to calculate the suicide rate of each year

```{r}
process_suicide_data = function( year_ ) {
    country_data = filter( r_suicide,
        year == year_, sex == 'male', age == '15-24 years' ) %>%
        select( country, year, 'gdp_for_year....' )
    smaller = select( r_suicide, year, sex, age, suicides_no, population ) %>%
        filter( year == year_ )
    get_sr = function( age_ ) {
        m = filter( smaller, sex == 'male', age == age_ )
        f = filter( smaller, sex == 'female', age == age_ )
        sr = 100000 * ( m$suicides_no + f$suicides_no ) / (m$population + f$population) %>% return
    }
    sr15 = get_sr( '15-24 years' )
    sr25 = get_sr( '25-34 years' )
    sr35 = get_sr( '35-54 years' )

    return( cbind( country_data, 
        sr_15_24 = sr15, 
        sr_25_34 = sr25,
        sr_35_54 = sr35 ) )
}
```

create new data frame

```{r}
data = data.frame( 
    country = c(),
    year = c(),
    region = c(),
    unemploy = c(),
    gdp_year = c(),
    sr_15_24 = c(),
    sr_25_34 = c(),
    sr_35_54 = c()
)
```
compute the suicide rate of each year and add those data to the new data frame
```{r}
for ( i in 1991:2017 ) {
    ue_data = select( r_unemploy, Country.Name, Region, paste('X', format(i), sep='') )
    sc_data = process_suicide_data( i )
    ue_data = cbind( ue_data, year=i )
    names( ue_data )[1] = 'country'
    result = merge( ue_data, sc_data, by = c('country', 'year') )
    names( result )[3:5] = c('region', 'unemploy', 'gdp_year')
    data = rbind( data, result )
}
```

print the new data frame

```{r, echo=FALSE}
head(data)
```

# Data Collecting

### Description

We (my teammate 王昊謙 and I) want to analyse the relation between 
word frequency and the number of examples in some of the most frequently used
dictionaries. Since we haven't thought of the details,
we simply compute the word frequency from a forum and count the examples in some dictionaries in this homework.

This program computes the word frequency of the post and comments on 
a forum of stack exchange 
([https://english.stackexchange.com/](https://english.stackexchange.com/)). 
It reads the posts from page 1 to the desired page number (it is set to 1 here for demo), with 50 posts in each page.

(I also implemented the program that scraps from other forums like reddit, but I didn't put it here since there're still some minor bugs)

However, since the program will be block
if it send too many requests in a short period: 

- I used `Sys.sleep(sec)` to limit the frequency of sending requests.
- When http_error 429 (too many requests) occurs,
the program will sleep and retry after one minute.


### Code

Here is the setup of some parameters.

```{r}
output_dir = './data/output.csv'
page_total = 1  # 50 posts in a page
index_url = 'https://english.stackexchange.com/questions?tab=Votes&pagesize=50&page='
base_url = 'https://english.stackexchange.com'
time_to_wait = 30 #sec
```

The function `get_urls` gets the urls of 50 posts from a page. 
And the function `get_text` gets all the content and comments from a post.

```{r}
get_urls <- function( page_num ) {

    # if an http error occur and the status code is 429 (too many requests),
    # retry after 1 minute
    repeat {
        r <- paste( index_url, page_num, sep='' ) %>% GET()
        if( http_error( r ) ) {
            if ( status_code( r ) == 429 ) wait() 
            else stop( status_code( r ) )
        }
        else break 
    }

    # read html from the response and get all the urls of posts
    read_html( r ) %>% 
        html_nodes( '#questions a.question-hyperlink' ) %>% 
        html_attr( 'href' ) %>% 
        return
}
get_text = function( url ) {

    # if an http error occur and the status code is 429 (too many requests),
    # retry after 1 minute
    repeat{ 
        r <- GET(paste( base_url, url, sep='' ))
        if( http_error( r ) ) {
            if ( status_code( r ) == 429 ) wait() 
            else stop( status_code( r ) )
        }
        else break 
    }

    # read html from the response 
    html <- read_html( r )

    # get the content of the post and all the comments
    post_text = html_nodes( html, ".post-text" ) %>% html_text()
    comment_text = html_nodes( html, ".comment-copy" ) %>% html_text()

    # combine all the text and replace the symbols and numbers with space
    c( post_text, comment_text ) %>% 
        str_replace_all( "[^a-zA-Z]", " ")  %>% 
        return
}
```

Here's the definition of the function `wait`. It just literally waits for 1 minute.

```{r}
wait <- function() {
    print( 'retry after 1 minutes...' )
    Sys.sleep( 60 )
}
```

The function `combine_data` takes the new text as parameter,
counts the frequency of words,
and adds the result to the table.

```{r}
combine_data = function( all.text, data ) {
    unnest_tokens( all.text, word, text ) %>%   # split the text
        count( word, sort=TRUE ) %>%            # count the frequency of words
        rbind( data ) %>%                       # combine the result with the data
        group_by( word ) %>%                    
        summarise( n = sum(n) ) %>%             # merge the rows with the same word
        return
}
```

And this is the main process of the program.

```{r}
data <- tibble( word=character(), n=integer() )  # initialize the table

# record the start time
start.time <- proc.time()

for( i in 1:page_total ) {

    # initialize a variable to store all the text
    all.text <- tibble( text = character() ) 
    start.time_loop <- proc.time() 

    # iterate through all 50 posts
    for( tail in get_urls(i) ) {

        # add the text from a post
        all.text = tibble( text=get_text( tail ) )  %>% 
            rbind( all.text )
    }

    # count the words in all 50 posts and update the table
    data <- combine_data( all.text, data )

    # wait for a while
    Sys.sleep( time_to_wait - (proc.time()-start.time_loop)[3] )

    #print the progress of the program (and calculate the estimated time left)
    cat( 'progress: ', i, '/', page_total, '  estimated: ', ((proc.time()-start.time)/i*(page_total-i))[3], 'sec\n', sep=''  )
}

# delete the words containing non-English characters
data <- data[which(!grepl("[^a-z]+", data$word)),] 

# sort the words by frequency and output the result
data <- data[order(data$n, decreasing=TRUE),]
write.csv(data,file=output_dir,row.names=FALSE) 
print( data )
```
